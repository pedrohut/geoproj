{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16245ac3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, extract, func, distinct\n",
    "from datetime import datetime, timedelta, date\n",
    "from pandas.tseries.offsets import Week, Day\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "### CREACION DE LA FUNCION DE SELECCION DE FECHAS ###\n",
    "\n",
    "def select_date(ref_week, year):\n",
    "    \"\"\"\n",
    "    Regresa la marca de tiempo de inicio y de fin de cada semana para la operacion del historial tecnico,\n",
    "    con las siguientes consideraciones: una semana de de ancho entre el inicio y fin,  los inicios inician el domingo y\n",
    "    terminan el sabado, la semana 1 y semamana 53 son irregulares por lo que terminan de acuerdo al año calendario.\n",
    "\n",
    "    Parámetros:\n",
    "    ref_week (float): numero de semana en curso.\n",
    "    año (float): año de semana en curso.\n",
    "\n",
    "    Retorna:\n",
    "    tupla(Timestamp, Timestamp, Timestamp): fecha de inicio, fecha de fin de acuerdo al año y semana de referencia, \n",
    "    Fecha de inicial.\n",
    "    \"\"\"\n",
    "\n",
    "    init_date = pd.Timestamp(year=year, month=1, day=1, hour=0, minute=0, second= 0)\n",
    "    if ref_week == 1: # Semana 1, se cruza entre el año anterior y el actual, semana irregular\n",
    "        cum_offset = pd.offsets.Week(weekday=init_date.weekday(), n=ref_week-1)\n",
    "        btw_offset = pd.offsets.Week(weekday=5)\n",
    "    elif ref_week == 53: # Semana 53, se cruza entre entre el año actual y el siguiente, semana irregular\n",
    "        cum_offset = pd.offsets.Week(weekday=6, n=ref_week-1)\n",
    "        btw_offset = pd.offsets.Week(weekday=(init_date + pd.offsets.YearEnd()).weekday()) \n",
    "    elif ref_week > 53:\n",
    "        raise ValueError('Solo existen 53 semanas maximo por año, favor de revisar')       \n",
    "    else:\n",
    "        cum_offset = pd.offsets.Week(weekday=6, n=ref_week-1)\n",
    "        btw_offset = pd.offsets.Week(weekday=5)\n",
    "    \n",
    "    adjust_hms = timedelta(hours=23, minutes=59, seconds=59) \n",
    "    # cum_offset es el numero de semanas acumuladas y desplazadas\n",
    "    # btw_offset es el numero de semanas desplazadas entre el time_start y time_end\n",
    "    # adjust_hms representa el desplazamiento de tiempo para el cierre del último día\n",
    "    \n",
    "    time_start = init_date + cum_offset\n",
    "    time_end = init_date + cum_offset +  btw_offset + adjust_hms \n",
    "\n",
    "    return time_start, time_end, init_date\n",
    "\n",
    "time_start, time_end, init_date = select_date(1, 2024)\n",
    "\n",
    "### IMPORTACION DE COORDENADAS DE GEOCERCAS DE LA BBDD A DF ###\n",
    "\n",
    "# Cargar el archivo .env\n",
    "load_dotenv(dotenv_path='data/.env')\n",
    "\n",
    "\n",
    "# Parámetros de conexión a la base de datos SQL Server\n",
    "dbuser = os.getenv('DBUSER')\n",
    "password = os.getenv('PASSWORD')\n",
    "server = os.getenv('SERVER')\n",
    "port = os.getenv('PORT')\n",
    "database1 = os.getenv('DATABASE1')\n",
    "database2 = os.getenv('DATABASE2')\n",
    "driver = os.getenv('DRIVER')\n",
    "\n",
    "\n",
    "# Cadenas de conexión\n",
    "connection_url1 = f'mssql+pyodbc://{dbuser}:{password}@{server}:{port}/{database1}?driver={driver}'\n",
    "connection_url2 = f'mssql+pyodbc://{dbuser}:{password}@{server}:{port}/{database2}?driver={driver}'\n",
    "\n",
    "\n",
    "# Crear engines\n",
    "engine1 = create_engine(connection_url1)\n",
    "engine2 = create_engine(connection_url2)\n",
    "\n",
    "\n",
    "# Crear varias instancias de MetaData y reflejar las tablas desde la base de datos\n",
    "metadata1 = MetaData()\n",
    "metadata2 = MetaData()\n",
    "\n",
    "metadata1.reflect(bind=engine1)\n",
    "metadata2.reflect(bind=engine2)\n",
    "\n",
    "\n",
    "\n",
    "# Seleccionar todas las filas de sus respectivas tablas\n",
    "HISTORIALTECNICO_FINAL = metadata1.tables['HISTORIALTECNICO_FINAL']\n",
    "AREAS_TBJ = metadata2.tables['AREAS_TBJ']\n",
    "\n",
    "\n",
    "# Construir los 'select' correspondientes\n",
    "\n",
    "\n",
    "not_economico = pd.read_csv('data/not_economico.csv').values.tolist()\n",
    "\n",
    "\n",
    "select1 = select(HISTORIALTECNICO_FINAL.c.Eco, \n",
    "                       HISTORIALTECNICO_FINAL.c.FechaI,\n",
    "                       HISTORIALTECNICO_FINAL.c.Rancho,\n",
    "                       HISTORIALTECNICO_FINAL.c.Tabla,\n",
    "                       HISTORIALTECNICO_FINAL.c.Longitud,\n",
    "                       HISTORIALTECNICO_FINAL.c.Latitud).where(\n",
    "    HISTORIALTECNICO_FINAL.c.FechaI.between(time_start, time_end),\n",
    "    HISTORIALTECNICO_FINAL.c.Eco.notin_(not_economico) \n",
    ")\n",
    "\n",
    "\n",
    "select2 = select(distinct(AREAS_TBJ.c.Fecha))\n",
    "\n",
    "\n",
    "# Conectar los 'select()' y 'engine' y obtener los datos los DataFrames correspondientes\n",
    "\n",
    "df1 = pd.read_sql(select1, engine1)\n",
    "df2 = pd.read_sql(select2, engine2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab7e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point,LineString , Polygon\n",
    "import shapely.geometry as geom\n",
    "\n",
    "### AGRUPACION DE COORDENADAS DE GEOCERCAS  ###\n",
    "\n",
    "\n",
    "# Crear una geometría Point para cada fila del DataFrame\n",
    "geometry = [Point(xy) for xy in zip(df1['Longitud'], df1['Latitud'])]\n",
    "\n",
    "# Crear un GeoDataFrame y asignar la columna de geometría\n",
    "gdf = gpd.GeoDataFrame(df1, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Crear la columna fecha sin fecha y hora\n",
    "gdf[\"Fecha\"] = gdf[\"FechaI\"].dt.date\n",
    "\n",
    "\n",
    "dissolved = gdf.dissolve(by=['Fecha','Eco','Tabla']).reset_index()\n",
    "# sjoined = dissolved.sjoin(catalog,  how='left', predicate='within')\n",
    "# dissolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6def7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATALOGO CON COORDENADAS Y HECTÁREAS ###\n",
    "\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "namelist = {'Hectareas': 'HasTot', 'Poligono': 'CoordsTabla', 'Nombre HT': 'Tabla', 'Rancho HT': 'Rancho'}\n",
    "\n",
    "\n",
    "catalog = pd.read_excel('data/CATALOGO DE GEOCERCAS.xlsx').dropna(subset=['Poligono'])\n",
    "catalog = catalog.rename(columns=namelist)\n",
    "catalog = catalog.query(\"Categoria == '03 Tabla'\")\n",
    "catalog['CoordsTabla'] = catalog['CoordsTabla'].apply(wkt.loads)\n",
    "catalog = gpd.GeoDataFrame(catalog, geometry='CoordsTabla', crs='EPSG:4326')\n",
    "\n",
    "### UNION ENTRE GEOCERCAR AGRUPADAS Y CATALOGO DE COORDENADAS ###\n",
    "\n",
    "merged = dissolved.merge(catalog,  how='left', left_on=['Tabla', 'Rancho'], right_on=['Tabla', 'Rancho'], validate = 'many_to_one').set_index(['Fecha','Eco','Tabla'])\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f77565",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERTIR LOS PUNTOS AGRUPADOS EN OTRAS GEOMETRIAS###\n",
    "\n",
    "\n",
    "from alphashape import alphashape\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "\n",
    "def generate_geom(x):\n",
    "    if x['geometry'].geom_type == 'Point':\n",
    "        return x['geometry']\n",
    "    elif x['geometry'].geom_type != 'Point':    \n",
    "        if x.name[2] == '-':\n",
    "            return LineString(x['geometry'].geoms)\n",
    "        elif x.name[2] != '-':\n",
    "            return alphashape(x['geometry'].geoms, 0)\n",
    "\n",
    "merged['gengeom'] = merged.apply(generate_geom, axis=1)\n",
    "merged['gengeom'].crs = 'EPSG:4326'\n",
    "\n",
    "### OBTENER LAS HECTAREAS TRABAJADAS ###\n",
    "\n",
    "merged['HasTbj'] = round(merged['gengeom'].to_crs('EPSG:32614').area/10000, 2)\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e96c297-ba9c-4048-9b8f-23fac4e01e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "['2024-01-01 00:00:00', '2024-01-02 00:00:00', '2024-01-03 00:00:00',\n",
       " '2024-01-04 00:00:00', '2024-01-05 00:00:00', '2024-01-06 00:00:00']\n",
       "Length: 6, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DAR EL FORMATO A LA TABLA PARA LA SALIDA ###\n",
    "\n",
    "order = ['Eco','Fecha', 'Codigo ERP','Tabla', 'HasTbj','HasTot','gengeom',\n",
    "         'UrlImagen', 'LaborUsoMaqID', 'ImplementoUsoMaqID','Activo']\n",
    "\n",
    "namelist = {'gengeom':'Geometria', 'Codigo ERP':'Codigo'}\n",
    "\n",
    "\n",
    "merged_queried = merged[(merged['gengeom'].geom_type == 'Polygon')&(merged['HasTot'].notna())].reset_index()\n",
    "merged_queried = merged_queried.reindex(columns=order).rename(columns=namelist)\n",
    "merged_queried.index = merged_queried.index.rename('HistorialID')\n",
    "\n",
    "merged_queried['Fecha'] = pd.to_datetime(merged_queried['Fecha'])\n",
    "first_date_df =  merged_queried['Fecha'].min() \n",
    "merged_queried['Fecha'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2bdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREACIÓN DE UNA FUNCION DE VALIDACION (PARA INTEGRIDAD DE DATOS) ###\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "df_dates =  pd.Series(pd.to_datetime(merged.index.get_level_values('Fecha').unique())) # Representan las fechas de las geometrias que van a ingresar a la BBDD \n",
    "# df_dates =  merged_queried['Fecha'].unique()\n",
    "bd_dates =  df2['Fecha'] # Representan las fechas de las geometrias que ya ingresaron a la BBDD\n",
    "\n",
    "unstb_date = pd.Timestamp.now().date() - (pd.offsets.Week(weekday=6) * 3) # Es la fecha/marca de tiempo a partir de donde ya se considera 'demasiado reciente' y los datos no son tan confiables todavia\n",
    "all_dates = pd.concat([bd_dates, df_dates]) # Representan una combinacion de las dos fechas: [no han ingresado] + [van a ingresar]  \n",
    "full_dates = pd.Series(pd.date_range(start=init_date, end=time_end, freq='D')) # Es un RANGO DE FECHAS GENERADO la fecha minima de la BBDD (si está posible, si no elige otra) y la fecha final seleccionada (NO DE LA BBDD)\n",
    "\n",
    "\n",
    "\n",
    "def validate_entry():  \n",
    "    \n",
    "    \"\"\"\n",
    "    Valida que las fechas ingresadas no estén en la base datos ANTES de que ingresen, evitando que la agregues,\n",
    "    si la fecha es demasiado reciente (cualquier superior a la productiva y dos hasta dos semanas anteriores),\n",
    "    o que alguno de los días de la semana productiva ya se encuentre en la base de datos,\n",
    "\n",
    "    Por otro lado también da el aviso si hay días no incluidos en la base de datos, los cuales se deben evaluar,\n",
    "    por ejemplo si es un día que no hubo operacion es normal que aparezca el aviso, si son varios y no hay razón es \n",
    "    posible que se haya saltado una semana la ejecucion del programa.\n",
    "\n",
    "    Parámetros:\n",
    "    None\n",
    "\n",
    "    Retorna:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    chklmt_df = any(df_dates >= unstb_date) # Retorna solo 'True' si existe AL MENOS una fecha que IGUALE O SUPERE a la fecha de referencia (EL LIMITE DONDE LAS FECHAS SE CONSIDERAN ESTABLES), de lo contrario retorna 'False'\n",
    "    bd_empty = bd_dates.empty # Retorna 'True' si la BBDD no contiene ningun registro, de lo contrario retorna 'False'\n",
    "    \n",
    "    chkdup_bd = any(all_dates.duplicated()) # Retorna 'True' si existe AL MENOS un duplicado, de lo contrario retorna 'False' (es decir no debe exitir ningún duplicado para que aparezca 'False')\n",
    "    chk_init =  any(df_dates.isin([init_date])) # Revisa que las fechas que van a ingresar a la BBDD tengan la fecha inicial (la primer fecha que de debería estar en la BBDD) \n",
    "    chk_miss  = all(full_dates.isin(all_dates)) # Retorna 'True' si al menos una fecha full_dates NO SE ENCUENTRA dentro de all_dates (la idea evitar posibles gaps  de fechas faltantes comparando, all_dates (fechas reales) con full_dates (fechas generadas))\n",
    "    dts_miss = np.array(full_dates[~full_dates.isin(all_dates)].dt.date)\n",
    "    if chklmt_df: \n",
    "        raise RuntimeError('Error: Has agregado una fecha demasiado reciente')\n",
    "   \n",
    "    elif bd_empty:\n",
    "         if not chk_init:\n",
    "            warnings.warn(f'No se encontró el {init_date} (fecha de inicio) en las fechas de hectareas, si la incluiste en tu primer selección ignora este mensaje, de lo contrario revisa tu selección y agregalá a la BBDD', RuntimeWarning)\n",
    "            # raise RuntimeError('Error: Debes agregar una fecha que contenga de inicio de año en una BBDD vacia')\n",
    "            \n",
    "    elif not bd_empty:\n",
    "        if chkdup_bd:\n",
    "            raise RuntimeError('Error: Alguna fecha que estas intentando agregar ya esta en la BBDD')\n",
    "        \n",
    "        if not chk_miss:\n",
    "            warnings.warn(f'Las fechas {dts_miss} no están presentes, si las incluiste pero fueron filtradas en el cálculo ignora este mensaje, pero si no se seleccionó, debes generar las fechas faltantes', RuntimeWarning)\n",
    "            # raise RuntimeError('Error: Hay fechas faltantes, haz de nuevo la seleccion con fechas continuas')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e6bed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### DAR SALIDA A LOS DATOS EN SQL O EN EXCEL ###\n",
    "\n",
    "\n",
    "# validate_entry()\n",
    "\n",
    "try:\n",
    "    validate_entry()\n",
    "\n",
    "except RuntimeError as err:\n",
    "    print(err)\n",
    "\n",
    "else:\n",
    "    merged_queried.to_wkt().to_sql('AREAS_TBJ', con=engine2, index=False, if_exists='append')\n",
    "    bd_dates = pd.read_sql(select2, engine2)['Fecha']\n",
    "    all_dates = pd.concat([bd_dates, df_dates])\n",
    "\n",
    "# merged_queried.to_excel('output.xlsx', index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
